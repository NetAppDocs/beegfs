---
sidebar: sidebar
permalink: beegfs-verified.html
keywords: Overview, EF600, NetApp, BeeGFS, Verified Architecture
summary: "Verified nodes and designs."
---

= Verified nodes and designs
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/


[.lead]
You can deploy BeeGFS on NetApp using validated BeeGFS file servers (file nodes) attached to block storage systems (block nodes) using the verified building block designs.

== Verified block node

In the highly competitive world of business, speed is everything. However, even the fastest supercomputer cannot meet expectations if it does not have equally fast storage to support it. The NetApp EF600 all-flash array gives customers consistent, near-real-time access to data while supporting any number of workloads simultaneously. To enable fast, continuous feeding of data to AI and HPC applications, EF600 storage systems deliver up to two million cached read IOPS, response times of under 100 microseconds, and 42GBps sequential read bandwidth in one enclosure. With 99.9999% reliability from EF600 storage systems, data for AI and HPC operations is available whenever and wherever it is needed.

The key benefits of EF600 storage systems include:

* *Accelerate time to Insight.* Enable blazing fast streaming of data to AI and other HPC applications with high- throughput, low-latency storage.
* *Future-proof your investment.* Quickly respond to changing workload demands and exponential data growth with a building block architecture that enables seamlessly scaling of performance and capacity as needed.
* *Maximize cost efficiency.* Reduce operating costs with high-density drives and price/performance optimized storage building blocks to ensure you can spend more on compute than storage.
* *Reduce risk and enable success.* Rely on a fully integrated, validated AI and HPC infrastructure from industry leaders to help gain a competitive edge.  Maximize productivity with 99.9999% availability.

== Verified file nodes

=== Lenovo ThinkSystem SR665 Server

The SR665 is a two-socket 2U server featuring PCIe 4.0.  When configured to meet the requirements of this NetApp Verified Architecture,  it provides ample performance to run BeeGFS file services in a configuration well balanced with the available of throughput and IOPs provided by the direct attached E-Series nodes.

For more information about the Lenovo SR665, see https://lenovopress.com/lp1269-thinksystem-sr665-server[Lenovo’s website^].

=== BeeGFS parallel file system

BeeGFS is a parallel file system with an architecture based on the following four main services:

* *Management service.* Registers and monitors all other services.
* *Storage service.* Stores the distributed user file contents known as data chunk files.
* *Metadata service.* Keeps track of the file system layout, directory, and file attributes, and so on.
* *Client service.* Mounts the file system to access the stored data. This design provides flexibility that is key to meeting diverse and evolving AI and HPC workloads. Use of NetApp EF-Series storage systems as the underlying block nodes supercharges BeeGFS storage and metadata services by offloading RAID and other storage tasks including drive monitoring and wear detection.

The key benefits of the BeeGFS parallel file system include:

* Allows optimization for diverse workloads within a single storage namespace.
+
Do your compute or GPU nodes each need to access a large number of small files? Do they each need to access a single large file? Do they all need to access the same set of small or large files? Don’t know? Many storage solutions are only good at some of these. BeeGFS does it all.

* Designed and developed for ease of use, straightforward installation, and simple management.
+
Eliminate complexity associated with traditional parallel and distributed file systems while taking full advantage of the performance benefits.

* Reduce client CPU overhead to facilitate network transfers and get data to the science faster by using remote direct memory access (RDMA) over IB.
+
For servers that don’t support RDMA, BeeGFS can serve files over TCP/IP and remote direct memory access (RDMA) concurrently ensuring no one is left out.

* Intelligently distributed file contents and metadata optimized for highly concurrent access.
+
Avoid fundamental architectural limitations imposed by the design of some storage solutions.

== Verified hardware design

The second-generation NetApp BeeGFS building block (shown in the following figure) uses two dual socket PCIe 4.0-capable servers for the BeeGFS file layer and two NetApp EF600 storage systems as the block layer.

These 8U building blocks more than double the performance of the https://www.netapp.com/pdf.html?item=/media/25445-nva-1156-design.pdf[NetApp first-generation BeeGFS building block^] design while adding support for high availability. Multiple building blocks are combined to create a BeeGFS parallel file system, which can span multiple datacenter racks if necessary. These building blocks are the hardware aspect of this NetApp Verified Architecture.

image:beegfs-design-image2.png[]

[NOTE]
Because each building block includes two BeeGFS file nodes, a minimum of two building blocks is required to establish quorum in the failover cluster. While it is possible to configure a two-node cluster, there are limitations to this configuration that might prevent a successful failover to occur in some scenarios.  If a two-node cluster is required,  it is also possible to incorporate a third device as a tiebreaker,  although that is not described in this design guide.

Each building block delivers high availability through a two-tier hardware design that separates fault domains for the file and block layers. Each tier can independently fail over providing increased resiliency and reducing the risk of cascading failures. The use of HDR InfiniBand in conjunction with NVMeOF provides high throughput and minimal latency between file and block nodes, with full redundancy and sufficient link oversubscription to avoid the disaggregated design becoming a bottleneck, even when the system is partially degraded.

The NetApp software-defined BeeGFS solution runs across all building blocks in the deployment. The first building block deployed must run BeeGFS management, metadata, and storage services (referred to as the base building block). All subsequent building blocks are configured through software to run BeeGFS metadata and storage services, or only storage services. The availability of different configuration profiles for each building block enables scaling of file system metadata or storage capacity and performance using the same underlying hardware platforms and building block design.

Up to five building blocks are combined into a standalone Linux HA cluster, ensuring a reasonable number of resources per cluster resource manager (Pacemaker),  and reducing the messaging overhead required to keep cluster members in sync (Corosync). A minimum of two building blocks per cluster is recommended to allow enough members to establish quorum. One or more of these standalone BeeGFS HA clusters are combined to create a BeeGFS file system (shown in the following figure) that is accessible to clients as a single storage namespace.

image:beegfs-design-image3.png[]

Although ultimately the number of building blocks per rack depends on the power and cooling requirements for a given site, the solution was designed so that up to five building blocks can deployed in a single 42U rack while still providing room for two 1U InfiniBand switches used for the storage/data network.  Each building block requires eight IB ports (four per switch for redundancy),  so five building blocks leaves half the ports on a 40- port HDR InfiniBand switch (like the NVIDIA QM8700) available to implement a fat-tree or similar nonblocking topology. This configuration ensures that the number storage or compute/GPU racks can be scaled up without worrying about networking bottlenecks.  Optionally,  an oversubscribed storage fabric can be used at the recommendation of the storage fabric vendor.

The following image shows an 80-node fat-tree topology.

image:beegfs-design-image4.png[]

By using Ansible as the deployment engine to deploy BeeGFS on NetApp, the entire environment is maintained using https://www.netapp.com/blog/deploying-beegfs-eseries/[modern infrastructure as code^] practices. This drastically simplifies what would otherwise be a complex system of systems, allowing administrators to define and adjust configuration all in one place and then verify that it is applied consistently regardless of how large the environment scales.
