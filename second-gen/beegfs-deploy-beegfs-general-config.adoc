---
sidebar: sidebar
permalink: beegfs-deploy-beegfs-general-config.html
keywords:
summary:
---

= Define general configuration
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
Learn how to define the general configuration for file and block nodes.

== Step 1: Define configuration for all building blocks
Define the configuration that applies to all building blocks, regardless of which eventual configuration profile  applied to them individually.

.Steps
. On your Ansible control node,  identify a directory used to store the Ansible inventory and playbook files that describe this BeeGFS deployment.
+
Unless otherwise noted,  all files and directories created in this and following sections  created relative to this directory.
+
While outside the scope of this document, storing the contents of the directory containing the inventory and playbook files describing your BeeGFS deployment in a source/version control system like BitBucket or Git is *strongly* recommended.
+
Creating a `.gitignore` file that ignores the packages/directory is also recommended to avoid storing large files in Git if Ansible is used to manage SANtricity OS software and other potentially large files.

. Create the following subdirectories: `host_vars`,  `group_vars`, and `packages`.

== Step 2: Specify configuration for individual file and block nodes

. Under `host_vars/`,  create a file for each BeeGFS file node named `<HOSTNAME>.yml `with the following content, paying special attention to the notes regarding content to populate for BeeGFS cluster IPs and host names ending in odd versus even numbers.
+
You might have noticed that initially the file node  interface names that do match up with what is listed here (such as ib0 or ibs1f0). These custom names  configured in a later section (see: `eseries_ib_base_udev_rules`).
+
....
ansible_host: “<MANAGEMENT_IP>”
eseries_ipoib_interfaces:  # Used to configure BeeGFS cluster IP addresses.
  - name: i1b
    address: 100.127.100. <NUMBER_FROM_HOSTNAME>/16
  - name: i4b
    address: 100.128.100. <NUMBER_FROM_HOSTNAME>/16
beegfs_ha_cluster_node_ips:
  - <MANAGEMENT_IP>
  - <i1b_BEEGFS_CLUSTER_IP>
  - <i4b_BEEGFS_CLUSTER_IP>
# NVMe over InfiniBand storage communication protocol information
# For odd numbered file nodes (i.e., h01, h03, ..):
eseries_nvme_ib_interfaces:
  - name: i1a
    address: 192.168.1.10/24
    configure: true
  - name: i2a
    address: 192.168.3.10/24
    configure: true
  - name: i3a
    address: 192.168.5.10/24
    configure: true
  - name: i4a
    address: 192.168.7.10/24
    configure: true
# For even numbered file nodes (i.e., h02, h04, ..):
# NVMe over InfiniBand storage communication protocol information
eseries_nvme_ib_interfaces:
  - name: i1a
    address: 192.168.2.10/24
    configure: true
  - name: i2a
    address: 192.168.4.10/24
    configure: true
  - name: i3a
    address: 192.168.6.10/24
    configure: true
  - name: i4a
    address: 192.168.8.10/24
    configure: true
....
+
[NOTE]
If you have already deployed the BeeGFS cluster, you must stop the cluster before adding or changing statically configured IP addresses including cluster IPs and IPs used for NVMe/IB. This is required to these changes take effect properly and do not disrupt cluster operations.

. Under `host_vars/`,  create a file for each BeeGFS block node named `<HOSTNAME>.yml `and populate with the following contentagain paying special attention to the notes regarding content to populate for storage array names ending in odd vs even numbers.
+
For each block node,  you only create one file and specify the `<MANAGEMENT_IP> `for one of the two controllers (usually the A one).
+
....
eseries_system_name: <STORAGE_ARRAY_NAME>
eseries_system_api_url: https://<MANAGEMENT_IP>:8443/devmgr/v2/
eseries_initiator_protocol: nvme_ib
# For odd numbered block nodes (i.e., a01, a03, ..):
eseries_controller_nvme_ib_port:
  controller_a:
    - 192.168.1.101
    - 192.168.2.101
    - 192.168.1.100
    - 192.168.2.100
  controller_b:
    - 192.168.3.101
    - 192.168.4.101
    - 192.168.3.100
    - 192.168.4.100
# For even numbered block nodes (i.e., a02, a04, ..):
eseries_controller_nvme_ib_port:
  controller_a:
    - 192.168.5.101
    - 192.168.6.101
    - 192.168.5.100
    - 192.168.6.100
  controller_b:
    - 192.168.7.101
    - 192.168.8.101
    - 192.168.7.100
    - 192.168.8.100
....

== Step 3: Specify common file and block node configuration

Ansible allows us to define configuration common to a group of hosts under `group_vars` in a file name that corresponds with the group. Among other benefits, this prevents repeating shared configuration in multiple places.

.About this task
Hosts can be in more than one group, and at runtime Ansible chooses what variables apply to a particular host based on its https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html[variable precedence^] rules.

Host-to-group assignments are defined in the actual Ansible inventory file, which is created towards the end of this section.

.Steps
. In Ansible any configuration,  you want to apply to all hosts that can be defined in a group called All.  Create the file `group_vars/all.yml` with the following content:
+
....
ansible_python_interpreter: /usr/bin/python3
beegfs_ha_ntp_server_pools:  # Modify the NTP server addressess if desired.
  - "pool 0.pool.ntp.org iburst maxsources 3"
  - "pool 1.pool.ntp.org iburst maxsources 3"
....

== Step 4: Specify common file node configuration

The shared configuration for file nodes is defined in a group called `ha_cluster`. The steps in this section build out the configuration that should be included in the `group_vars/ha_cluster.yml` file.

.Steps
. At the top of the file,  define a few defaults, including the password that should be used to become the sudo user on the file nodes.
+
....
### ha_cluster Ansible group inventory file.
# Place all default/common variables for BeeGFS HA cluster resources below.
### Cluster node defaults
ansible_ssh_user: root
ansible_become_password: <PASSWORD>
eseries_ipoib_default_hook_templates:
  - 99-multihoming.j2 # This is required when configuring additional static IPs (for example cluster IPs) when multiple IB ports are in the same IPoIB subnet.
# If the following options are specified, then Ansible will automatically reboot nodes when necessary for changes to take effect:
eseries_common_allow_host_reboot: true
eseries_common_reboot_test_command: "systemctl --state=active,exited | grep eseries_nvme_ib.service"
....
+
[NOTE]
Particularly for production environments,  do note store passwords in plain text and instead use https://docs.ansible.com/ansible/latest/user_guide/vault.html[Ansible Vault^] or the `--ask-become-pass` option when running the playbook.  If the `ansible_ssh_user` is already root, then omitting the `ansible_become_password` is another option.

. Optionally, configure a name for the high-availability (HA) cluster and specify a user that should be created for intracluster communication.
+
If the private IP addressing scheme is being modified, the default `beegfs_ha_mgmtd_floating_ip` needs to be updated.
+
Note this must match what is configured in a later section for the BeeGFS Management resource group.

. Specify one or more emails that should receive alerts for cluster events using `beegfs_ha_alert_email_list`.
+
....
### Cluster information
# The following variables should be adjusted depending on the desired configuration:
beegfs_ha_cluster_name: hacluster                  # BeeGFS HA cluster name.
beegfs_ha_cluster_username: hacluster              # BeeGFS HA cluster username.
beegfs_ha_cluster_password: hapassword             # BeeGFS HA cluster username's password.
beegfs_ha_cluster_password_sha512_salt: randomSalt # BeeGFS HA cluster username's password salt.
beegfs_ha_mgmtd_floating_ip: 100.127.101.0         # BeeGFS management service IP address.
# Email Alerts Configuration
beegfs_ha_enable_alerts: True
beegfs_ha_alert_email_list: ["email@example.com"]  # E-mail recipient list for notifications when BeeGFS HA resources change or fail.  Often a distribution list for the team responsible for managing the cluster.
beegfs_ha_alert_conf_ha_group_options:
      mydomain: “example.com”
# The mydomain parameter specifies the local internet domain name. This is optional when the cluster nodes have fully qualified hostnames (i.e. host.example.com).
# Adjusting the following parameters is optional:
beegfs_ha_alert_timestamp_format: "%Y-%m-%d %H:%M:%S.%N" #%H:%M:%S.%N
beegfs_ha_alert_verbosity: 3
#  1) high-level node activity
#  3) high-level node activity + fencing action information + resources (filter on X-monitor)
#  5) high-level node activity + fencing action information + resources
....
+
[NOTE]
While seemingly redundant here, `beegfs_ha_mgmtd_floating_ip` is important when scaling the BeeGFS file system beyond a single HA cluster. Subsequent HA clusters are deployed without an additional BeeGFS management service and point at the management service provided by the first cluster.
+
. Configure https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters[fencing^].
+
By default,  fencing is enabled, but you need to configure a fencing agent. The output shows examples configuring common fencing agents (choose one). The `<HOSTNAME>` specified in the `pcmk_host_map` or `pcmk_host_list` must correspond with the hostname in the Ansible inventory.

** Although the BeeGFS cluster can be deployed and run without fencing, this is not supported, particularly in production.
+
This is largely to ensure when BeeGFS services including any resource dependencies like block devices failover due to an issue, there is no risk of concurrent access by multiple nodes that result in file system corruption or other undesirableunexpected behavior.
** If fencing must be disabled,  refer to the general notes in the BeeGFS HA role’s getting started guide and set `beegfs_ha_cluster_crm_config_options[“stonith-enabled”]` to false in `ha_cluster.yml`.
** There are multiple node- level fencing devices available, and the BeeGFS HA role can configure any fencing agent available in the Red Hat HA package repository.
+
When possible,  a fencing agent that works through the uninterruptible power supply (UPS) or rack power distribution unit (rPDU) because some fencing agents such as the baseboard management controller (BMC) or other lights-out devices that are built into the server not respond to the fence request under certain failure scenarios.
+
....
### Fencing configuration:
# OPTION 1: To enable fencing using APC Power Distribution Units (PDUs):
beegfs_ha_fencing_agents:
 fence_apc:
   - ipaddr: <PDU_IP_ADDRESS>
     login: <PDU_USERNAME>
     passwd: <PDU_PASSWORD>
     pcmk_host_map: "<HOSTNAME>:<PDU_PORT>,<PDU_PORT>;<HOSTNAME>:<PDU_PORT>,<PDU_PORT>"
# OPTION 2: To enable fencing using the Redfish APIs provided by the Lenovo XCC (and other BMCs):
redfish: &redfish
  username: <BMC_USERNAME>
  password: <BMC_PASSWORD>
  ssl_insecure: 1 # If a valid SSL certificate is not available specify “1”.
beegfs_ha_fencing_agents:
  fence_redfish:
    - pcmk_host_list: <HOSTNAME>
      ip: <BMC_IP>
      <<: *redfish
    - pcmk_host_list: <HOSTNAME>
      ip: <BMC_IP>
      <<: *redfish
# For details on configuring other fencing agents see https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-fencing-configuring-and-managing-high-availability-clusters.
....
+
. Enable pereformance tuning.
+
As part of the performance benchmark testing used to verify this NetApp Verified Architecture,  several optional adjustments must be made to the block device and virtual memory subsystem configuration on the file nodes.
+
While many users find these generally work well, further improving performance for a particular workload is possible by further tuning. As such,  these recommendations are included in the BeeGFS role but not enabled by default to ensure users are aware of the tuning applied to their file system.
+
To enable performance tuning,  specify:
+
....
### Performance Configuration:
beegfs_ha_enable_performance_tuning: True
....
+
[NOTE]
For a comprehensive list of available tuning parameters that can be adjusted,  see the Performance Tuning Defaults section of the https://github.com/netappeseries/beegfs/tree/master/roles/beegfs_ha_7_2/defaults/main.yml[BeeGFS ^]HA role.  The default values can be overridden for all nodes in the cluster in this file or the `host_vars` file for an individual node.

. To allow full 200Gb/HDR connectivity between block and file nodes the Open Subnet Manager (OpenSM) package from the Mellanox Open Fabrics Enterprise Distribution (MLNX_OFED) must be used as the inbox `opensm` package does not support the necessary virtualization functionality.
+
Although deployment using Ansible is supported, the desired packages must first be downloaded to the Ansible control node used to run the BeeGFS role.
+
.. Download the packages for the version of OpenSM listed in the technology requirements section from Mellanox’s website to the `packages/` directory using curl or the tool of choice example:
+
....
curl -o packages/opensm-libs-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
curl -o packages/opensm-5.9.0. MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
....
+
.. Populate the following in `group_vars/ha_cluster.yml` (adjust packages as needed):
+
....
### OpenSM package and configuration information
eseries_ib_opensm_allow_upgrades: true
eseries_ib_opensm_skip_package_validation: true
eseries_ib_opensm_rhel_packages: []
eseries_ib_opensm_custom_packages:
  install:
    - files:
        add:
          "packages/opensm-libs-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm": "/tmp/"
          "packages/opensm-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm": "/tmp/"
    - packages:
        add:
          - /tmp/opensm-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
          - /tmp/opensm-libs-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
  uninstall:
    - packages:
        remove:
          - opensm
          - opensm-libs
      files:
        remove:
          - /tmp/opensm-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
          - /tmp/opensm-libs-5.9.0.MLNX20210617.c9f2ade-0.1.54103.x86_64.rpm
eseries_ib_opensm_options:
  virt_enabled: "2"
....

. Configure the udev rule to ensure consistent mapping of logical InfiniBand port identifiers to underlying PCIe devices.
+
The udev rule must be unique to the PCIe topology of each server platform used as a BeeGFS file node.
+
Use the following values for verified file nodes:
+
....
### Ensure Consistent Logical IB Port Numbering
# Name of the udev rule to create (do not modify):
eseries_ib_base_udev_name: 99-beegfs-ib.rules
# OPTION 1: Lenovo SR665 PCIe address-to-logical IB port mapping:
eseries_ib_base_udev_rules:
  "0000:41:00.0": i1a
  "0000:41:00.1": i1b
  "0000:01:00.0": i2a
  "0000:01:00.1": i2b
  "0000:a1:00.0": i3a
  "0000:a1:00.1": i3b
  "0000:81:00.0": i4a
  "0000:81:00.1": i4b

# Note: At this time no other x86 servers have been qualified. Configuration for future qualified file nodes will be added here.
....

. Update the metadata target selection algorithm if desired.
+
....
beegfs_ha_beegfs_meta_conf_ha_group_options:
  tuneTargetChooser: randomrobin
....
+
[NOTE]
In our verification testing,  `randomrobin` was typically used to test files were evenly distributed across all BeeGFS storage targets during https://doc.beegfs.io/latest/advanced_topics/benchmark.html[performance benchmarking^].  With real world use this might cause lowernumbered targets to fill up faster than higher numbered targets. Omitting this and just using the default `randomized` value has been shown to provide good performance while still utilization all available targets.

== Step 5: Specify common block node configuration

The shared configuration for block nodes is defined in a group called `eseries_storage_systems`.

.About this task
The steps in this section build out the configuration that should be included in the `group_vars/ eseries_storage_systems.yml` file.

.Steps
. Normally,  Ansible uses SSH to connect to managed hosts, but in the case of the NetApp E-Series storage systems used as block nodes,  the modules use the REST API for communication. To facilitate this,  you need to set the Ansible connection to local, provide the system password, and specify if SSL certificates should be verified. At the top of the file add:
+
....
### eseries_storage_systems Ansible group inventory file.
# Place all default/common variables for NetApp E-Series Storage Systems here:
ansible_connection: local
eseries_system_password: <PASSWORD>
eseries_validate_certs: false
....
+
[NOTE]
Listing any passwords in plaintext is not recommended. Use Ansible vault or provide the `eseries_system_password` when running Ansible using `--extra-vars`.
+
. This NetApp Verified Architecture recommends specific versions of the E-Series SANtricity OS controller software and NVSRAM.
+
To ensure optimal performance, install the versions listed under the technology requirements section for block nodes.
+
Download the corresponding https://mysupport.netapp.com/site/products/all/details/eseries-santricityos/downloads-tab[files^] from the https://mysupport.netapp.com/site/products/all/details/eseries-santricityos/downloads-tab[NetApp Support site^] and either upgrade manually or include them in the `packages/` directory of the Ansible control node and populate the following in `eseries_storage_systems.yml `to upgrade using Ansible:
+
....
# Firmware, NVSRAM, and Drive Firmware (modify the filenames as needed):
eseries_firmware_firmware: "packages/RCB_11.70.2_6000_61b1131d.dlp"
eseries_firmware_nvsram: "packages/N6000-872834-D06.dlp"
eseries_drive_firmware_firmware_list:
  - "packages/D_MZWLJ3T8HBLS-0G5_30604635_NA51_XXXX_000.dlp"
....
+
. NetApp recommends installing the latest drive firmware available.  Download the corresponding https://mysupport.netapp.com/NOW/download/tools/diskfw_eseries/[files^] for the drives installed in your block nodes from the NetApp Support site either upgrade manually or include them in the `packages/` directory of the Ansible control node and populate the following in `eseries_storage_systems.yml `to upgrade using Ansible:
+
....
eseries_drive_firmware_firmware_list:
  - "packages/<FILENAME>.dlp"
eseries_drive_firmware_upgrade_drives_online: true
....
+
[NOTE]
Setting `eseries_drive_firmware_upgrade_drives_online` to `false` will speed up the upgrade but should not be done until after BeeGFS is deployed because it requires stopping all I/O to the drives before the upgrade to avoid application errors.  Because performing an online drive firmware upgrade before configuring volumes is still quick, always setting this to `true` is recommended to avoid issues later.
+
. Several changes to the global configuration are recommended to optimize performance for this NetApp Verified Architecture.
+
....
# Global Configuration Defaults
eseries_system_cache_block_size: 32768
eseries_system_cache_flush_threshold: 80
eseries_system_default_host_type: linux dm-mp
eseries_system_autoload_balance: disabled
eseries_system_host_connectivity_reporting: disabled
eseries_system_controller_shelf_id: 99 # Required.
....
+
. Specify parameters to optimal volume provisioning and behavior.
+
....
# Storage Provisioning Defaults
eseries_volume_size_unit: pct
eseries_volume_read_cache_enable: true
eseries_volume_read_ahead_enable: false
eseries_volume_write_cache_enable: true
eseries_volume_write_cache_mirror_enable: true
eseries_volume_cache_without_batteries: false
eseries_storage_pool_usable_drives: "99:0,99:23,99:1,99:22,99:2,99:21,99:3,99:20,99:4,99:19,99:5,99:18,99:6,99:17,99:7,99:16,99:8,99:15,99:9,99:14,99:10,99:13,99:11,99:12"
....
+
[NOTE]
The value specified for `eseries_storage_pool_usable_drives` is specific to NetApp EF600 block nodes and controls the order in which drives are assigned to new volume groups. This ordering the I/O to each group is evenly distributed across backend drive channels.
