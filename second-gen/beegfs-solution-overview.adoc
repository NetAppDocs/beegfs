---
sidebar: sidebar
permalink: beegfs-solution-overview.html
keywords: Overview, EF600, NetApp, BeeGFS, Verified Architecture
summary: "The second generation NetApp Verified Architecture (NVA) provides customers with a shared-disk HA architecture powered by BeeGFS file servers and NetApp E-Series and EF-Series storage systems."
---

= Second generation overview
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/


[.lead]
The second generation NetApp Verified Architecture (NVA) provides customers with a shared-disk HA architecture powered by BeeGFS file servers and NetApp E-Series and EF-Series storage systems.

The second generation NVA takes advantage of the performance density delivered by the latest enterprise server and storage hardware and network speeds, requiring file nodes that feature dual AMD EPYC 7003 “Milan” processors and support for PCIe 4.0 with direct connects using 200Gb (HDR) InfiniBand to block nodes that provide end-to-end NVMe and NVMeOF using the NVMe/IB protocol.

The NetApp Verified Architecture describes how to design and deploy BeeGFS on NetApp using validated BeeGFS file servers (referred to throughout as file nodes) attached to block storage systems (referred to throughout as block nodes) using the second generation of the NetApp BeeGFS building block design.

A benefit of parallel file systems such as BeeGFS is their ability to be deployed and scaled out in different ways depending on the storage requirements. For example, use cases primarily featuring lots of small files will benefit from extra metadata performance and capacity, whereas use cases featuring fewer larger files might favor more storage capacity and performance for actual file contents. The fact that there are multiple considerations that impact different dimensions of the parallel file system deployment adds complexity to designing, deploying, and managing the parallel file system.

To address these challenges, NetApp has designed a standard building block architecture that is used to scale out each of these dimensions. Typically, BeeGFS building blocks are deployed in one of three configuration profiles:

* A single base building block, including BeeGFS management, metadata, and storage services
* A BeeGFS metadata + storage building block
* A BeeGFS storage only building block

The only (optional) hardware change between these three options is the use of smaller drives for BeeGFS metadata. Otherwise, all configuration changes are applied through software, and the use of Ansible as the deployment engine makes it straightforward to set the profile desired for a particular building block.

This approach provides administrators the flexibility needed to quickly adapt to diverse and shifting workload requirements while using a scalable building block architecture and the latest available hardware.

In addition, the BeeGFS on NetApp solution features a https://www.netapp.com/blog/high-availability-beegfs/[shared-disk HA design^] that helps reduce cost without sacrificing performance or reliability, requiring enterprise-grade block nodes for data durability.

== Use case summary

The BeeGFS on NetApp solution applies to the following use cases:

* AI including ML, DL, NLP, and NLU
* HPC including applications accelerated by MPI and other distributed computing techniques
* Application workloads characterized by:
** Reading or writing to files larger than 1GB
** Reading or writing to the same file by multiple clients (10s, 100s, and 1000s)
* Multiterabyte or multipetabyte datasets
* Environments that need a single storage namespace optimizable for a mix of large and small files

== Verified block nodes: NetApp EF600 storage system

In the highly competitive world of business, speed is everything. However, even the fastest supercomputer cannot meet expectations if it does not have equally fast storage to support it. The NetApp EF600 all-flash array gives customers consistent, near-real-time access to data while supporting any number of workloads simultaneously. To enable fast, continuous feeding of data to AI and HPC applications, EF600 storage systems deliver up to two million cached read IOPS, response times of under 100 microseconds, and 42GBps sequential read bandwidth in one enclosure. With 99.9999% reliability from EF600 storage systems, data for AI and HPC operations is available whenever and wherever it is needed.

==== Key benefits

The key benefits of EF600 storage systems include:

* *Accelerate time to Insight.* Enable blazing fast streaming of data to AI and other HPC applications with high- throughput, low-latency storage.
* *Future-proof your investment.* Quickly respond to changing workload demands and exponential data growth with a building block architecture that enables seamlessly scaling of performance and capacity as needed.
* *Maximize cost efficiency.* Reduce operating costs with high-density drives and price/performance optimized storage building blocks to ensure you can spend more on compute than storage.
* *Reduce risk and enable success.* Rely on a fully integrated, validated AI and HPC infrastructure from industry leaders to help gain a competitive edge.  Maximize productivity with 99.9999% availability.

== Verified file nodes: Lenovo ThinkSystem SR665 Server

The SR665 is a two-socket 2U server featuring PCIe 4.0.  When configured to meet the requirements of this NetApp Verified Architecture,  it provides ample performance to run BeeGFS file services in a configuration well balanced with the available of throughput and IOPs provided by the direct attached E-Series nodes.

For more information about the Lenovo SR665, see https://lenovopress.com/lp1269-thinksystem-sr665-server[Lenovo’s website^].

== BeeGFS parallel file system

BeeGFS is a parallel file system with an architecture based on the following four main services:

* *Management service.* Registers and monitors all other services.
* *Storage service.* Stores the distributed user file contents known as data chunk files.
* *Metadata service.* Keeps track of the file system layout, directory, and file attributes, and so on.
* *Client service.* Mounts the file system to access the stored data. This design provides flexibility that is key to meeting diverse and evolving AI and HPC workloads. Use of NetApp EF-Series storage systems as the underlying block nodes supercharges BeeGFS storage and metadata services by offloading RAID and other storage tasks including drive monitoring and wear detection.

=== Key benefits

The key benefits of the BeeGFS parallel file system include:

* Allows optimization for diverse workloads within a single storage namespace.
+
Do your compute or GPU nodes each need to access a large number of small files? Do they each need to access a single large file? Do they all need to access the same set of small or large files? Don’t know? Many storage solutions are only good at some of these. BeeGFS does it all.

* Designed and developed for ease of use, straightforward installation, and simple management.
+
Eliminate complexity associated with traditional parallel and distributed file systems while taking full advantage of the performance benefits.

* Reduce client CPU overhead to facilitate network transfers and get data to the science faster by using remote direct memory access (RDMA) over IB.
+
For servers that don’t support RDMA, BeeGFS can serve files over TCP/IP and remote direct memory access (RDMA) concurrently ensuring no one is left out.

* Intelligently distributed file contents and metadata optimized for highly concurrent access.
+
Avoid fundamental architectural limitations imposed by the design of some storage solutions.

== BeeGFS on NetApp

While the community edition of BeeGFS can be used free of charge, the enterprise edition requires purchasing a professional support subscription contract from a partner like NetApp.  The enterprise edition allows use of several additional features including resiliency, quota enforcement, and storage pools.

The BeeGFS on NetApp solution expands the functionality of the BeeGFS enterprise edition by creating a fully integrated solution with NetApp hardware and enabling high availability based on NetApp E-Series and EF-Series storage systems using a shared-disk HA architecture.

The following figure compares the shared-nothing and shared-disk HA architectures.

image:beegfs-design-image1.png[Error: Missing Graphic Image]

BeeGFS on NetApp is delivered and deployed using Ansible automation hosted on https://github.com/netappeseries/beegfs/[GitHub^] and https://galaxy.ansible.com/netapp_eseries/beegfs[Ansible Galaxy^] (see the  <<xref>> section for more details).  Although it is primarily tested with the hardware used to assemble the BeeGFS building blocks described in this NetApp Verified Architecture, it can be configured to run on virtually any x86 based server using a supported Linux distribution.

=== Key benefits

In addition to being backed and supported by a leading on- premises and cloud storage provider, the key benefits of using BeeGFS on NetApp include the following:

* Availability of verified hardware designs providing full integration of hardware and software components to ensure predicable performance and reliability.
* Deployed and managed using Ansible for simplicity and consistency at scale.
* Monitoring and observability provided using the https://www.netapp.com/blog/monitoring-netapp-eseries/[E-Series Performance Analyzer and BeeGFS plugin^].
* High availability featuring a shared-disk architecture that provides data durability and availability.
* Support for https://www.netapp.com/blog/kubernetes-meet-beegfs/[modern workload management and orchestration^] using containers and Kubernetes.

== Verified hardware designs: Second-generation building block

The second-generation NetApp BeeGFS building block (shown in the following figure) uses two dual socket PCIe 4.0- capable servers for the BeeGFS file layer and two NetApp EF600 storage systems as the block layer.

These 8U building blocks more than double the performance of the https://www.netapp.com/pdf.html?item=/media/25445-nva-1156-design.pdf[NetApp first-generation BeeGFS building block^] design while adding support for high availability.  Multiple building blocks are combined to create a BeeGFS parallel file system, which can span multiple datacenter racks if necessary.  These building blocks are the hardware aspect of this NetApp Verified Architecture.

image:beegfs-design-image2.png[Error: Missing Graphic Image]

[NOTE]
Because each building block includes two BeeGFS file nodes, a minimum of two building blocks is required to establish quorum in the failover cluster. While it is possible to configure a two-node cluster, there are limitations to this configuration that might prevent a successful failover to occur in some scenarios.  If a two-node cluster is required,  it is also possible to incorporate a third device as a tiebreaker,  although that is not described in this design guide.

Each building block delivers high availability through a two-tier hardware design that separates fault domains for the file and block layers. Each tier can independently fail over providing increased resiliency and reducing the risk of cascading failures. The use of HDR InfiniBand in conjunction with NVMeOF provides high throughput and minimal latency between file and block nodes, with full redundancy and sufficient link oversubscription to avoid the disaggregated design becoming a bottleneck, even when the system is partially degraded.

The NetApp software-defined BeeGFS solution runs across all building blocks in the deployment. The first building block deployed must run BeeGFS management, metadata, and storage services (referred to as the base building block). All subsequent building blocks are configured through software to run BeeGFS metadata and storage services, or only storage services. The availability of different configuration profiles for each building block enables scaling of file system metadata or storage capacity and performance using the same underlying hardware platforms and building block design.

Up to five building blocks are combined into a standalone Linux HA cluster, ensuring a reasonable number of resources per cluster resource manager (Pacemaker),  and reducing the messaging overhead required to keep cluster members in sync (Corosync). A minimum of two building blocks per cluster is recommended to allow enough members to establish quorum. One or more of these standalone BeeGFS HA clusters are combined to create a BeeGFS file system (shown in the following figure) that is accessible to clients as a single storage namespace.

image:beegfs-design-image3.png[Error: Missing Graphic Image]

Although ultimately the number of building blocks per rack depends on the power and cooling requirements for a given site, the solution was designed so that up to five building blocks can deployed in a single 42U rack while still providing room for two 1U InfiniBand switches used for the storage/data network.  Each building block requires eight IB ports (four per switch for redundancy),  so five building blocks leaves half the ports on a 40- port HDR InfiniBand switch (like the NVIDIA QM8700) available to implement a fat-tree or similar nonblocking topology. This configuration ensures that the number storage or compute/GPU racks can be scaled up without worrying about networking bottlenecks.  Optionally,  an oversubscribed storage fabric can be used at the recommendation of the storage fabric vendor.

The following image shows an 80-node fat-tree topology.

image:beegfs-design-image4.png[Error: Missing Graphic Image]

By using Ansible as the deployment engine to deploy BeeGFS on NetApp, the entire environment is maintained using https://www.netapp.com/blog/deploying-beegfs-eseries/[modern infrastructure as code^] practices. This drastically simplifies what would otherwise be a complex system of systems, allowing administrators to define and adjust configuration all in one place and then verify that it is applied consistently regardless of how large the environment scales.
